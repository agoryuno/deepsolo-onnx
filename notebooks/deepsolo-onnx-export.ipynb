{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_yzGnfLgJaA"
      },
      "outputs": [],
      "source": [
        "%% capture\n",
        "\n",
        "!git clone https://github.com/agoryuno/deepsolo-onnx\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "os.chdir('deepsolo-onnx')\n",
        "\n",
        "!pip install torch torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!python setup.py sdist bdist_wheel\n",
        "!pip install dist/*.whl\n",
        "\n",
        "shutil.copy('configs/Base_det_export.yaml', '/content')\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "!rm -rf deepsolo-onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN8kGfDGFSnr"
      },
      "outputs": [],
      "source": [
        "# This is for quick reloads during development- DON\"T EXECUTE\n",
        "assert False\n",
        "!pip install --force-reinstall --no-deps git+https://github.com/agoryuno/deepsolo-onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHb0sSBxouvf"
      },
      "outputs": [],
      "source": [
        "from DeepSolo.onnx_model import SimpleONNXReadyModel\n",
        "\n",
        "# Specify the path to the DeepSolo checkpoint you want to export\n",
        "# you can download one from the main DeepSolo repo: \n",
        "# https://github.com/ViTAE-Transformer/DeepSolo\n",
        "#\n",
        "CHECKPOINT = \"/content/drive/MyDrive/deepsolo/vitaev2-s_pretrain_synth-tt-mlt-13-15-textocr.pth\"\n",
        "\n",
        "# This is the default config file\n",
        "# you probably should leave it alone\n",
        "#\n",
        "CONFIG = \"/content/Base_det_export.yaml\"\n",
        "\n",
        "model = SimpleONNXReadyModel(CONFIG, CHECKPOINT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Qlh0RmADOJ0W"
      },
      "outputs": [],
      "source": [
        "# Creating a fake \"image\" to set the input-output dimensions\n",
        "# of the exported model's data\n",
        "#\n",
        "# \n",
        "\n",
        "# Input and output image dimensions - these are fixed \n",
        "# for the exported ONNX model, so choose wisely\n",
        "#\n",
        "DIMS = (512,512)\n",
        "\n",
        "import torch\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from DeepSolo.adet.utils.visualizer import TextVisualizer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# The number of channels in an image\n",
        "# This can be left at default\n",
        "#\n",
        "CHANNELS = 3\n",
        "\n",
        "img = np.random.randint(0, 255, (CHANNELS, *DIMS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os6_WaaYit3h"
      },
      "outputs": [],
      "source": [
        "import torch.onnx\n",
        "import torch\n",
        "\n",
        "img_t = torch.from_numpy(img).permute(2, 0, 1)\n",
        "\n",
        "torch.onnx.export(model.model,\n",
        "           [img_t],\n",
        "            #[{\"image\": img_t}],\n",
        "           \"deepsolo.onnx\",\n",
        "           export_params=True,\n",
        "           do_constant_folding=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0VJWc_eKS80"
      },
      "outputs": [],
      "source": [
        "# Everything below requires the default DeepSolo model to work\n",
        "\n",
        "predictions = model(img)\n",
        "metadata = MetadataCatalog.get(\"__unused\")\n",
        "instance_mode = ColorMode.IMAGE\n",
        "visualizer = TextVisualizer(img, metadata, instance_mode=instance_mode, cfg=model.cfg)\n",
        "vis_output = None\n",
        "instances = predictions[\"instances\"].to(torch.device(\"cpu\"))\n",
        "vis_output = visualizer.draw_instance_predictions(predictions=instances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYRZ4dM0Ljyn"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "out_img = vis_output.get_image()[:, :, ::-1]\n",
        "plt.imshow(out_img)\n",
        "plt.axis('off')  # Hide axes\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcmU5dvdGklb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "ctrl_pnts = instances.ctrl_points.numpy()\n",
        "scores = instances.scores.tolist()\n",
        "recs = instances.recs\n",
        "bd_points = np.asarray(instances.bd)\n",
        "\n",
        "for ctrl_pnt, score, rec, bd in zip(ctrl_pnts, scores, recs, bd_points):\n",
        "  if bd is not None:\n",
        "    # the bounding polygon\n",
        "    bd = np.hsplit(bd, 2)\n",
        "    bd = np.vstack([bd[0], bd[1][::-1]])\n",
        "\n",
        "    # decode text\n",
        "    txt = visualizer._ctc_decode_recognition(rec)\n",
        "    print (txt)\n",
        "\n",
        "    # centerline\n",
        "    line = visualizer._process_ctrl_pnt(ctrl_pnt)\n",
        "    print (line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UscN5hWpWleB",
        "outputId": "fde7e049-2f81-4a5f-96a3-daad87d060fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DeepSolo.onnx_model.ViTAEPredictor"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(model.model)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
